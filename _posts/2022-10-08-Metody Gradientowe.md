---
layout: post
title: Iteracje metodÄ… Jacobiego oraz Gaussa-Seidla
subtitle: Metody numeryczne
cover-img: /assets/img/geometry.jpg
thumbnail-img: /assets/img/menuin.jpg
share-img: /assets/img/geometry.jpg
tags: [metody numeryczne, Jacobiego, Gaussa-Seidla, iteracje]
---

**1. WstÄ™p**

ZaletÄ… metod iteracyjnych jest moÅ¼liwoÅ›Ä‡ wyznaczenia przybliÅ¼enia rozwiÄ…zania z zadanÄ… dokÅ‚adnoÅ›ciÄ…, niekiedy kosztem istotnie mniejszym od kosztu metod skoÅ„czonych. Dla niektÃ³rych zadaÅ„ metody iteracyjne sÄ… wiÄ™c efektywniejsze. JednÄ… z najprostszych metod iteracyjnych jest metoda iteracji prostej. Polega ona na przejÅ›ciu od danego ukÅ‚adu rÃ³wnaÅ„ liniowych do rÃ³wnowaÅ¼nego (tzn. majÄ…cego te same rozwiÄ…zania) ukÅ‚adu: $\mathcal{x}=Bx+c$.

**2. Metoda Jacobiego**
Metoda Jacobiego jest metodÄ… iteracyjnÄ… i pozwala nam obliczyÄ‡ ukÅ‚ad n rÃ³wnaÅ„ z n niewiadomymi $\mathcal{Ax}=b$.


**2.1 RozwiÄ…zanie analityczne:**

$\mathcal{
f(x)=\left\{ \begin{array}{lr} x+1 & dla \ x\in(-\infty;0) \\ x-1 & dla \ x\in\langle0;+\infty) \end{array}\right
}$

1. PrzyjÄ…Ä‡ punkt startowy $\mathcal{x_0}$, $\mathcal{C^2}$.dÅ‚ugoÅ›Ä‡ kroku e, wspÃ³Å‚czynnik redukcji kroku ğ’‚ < ğŸ, limit liczby redukcji kroku k (np. ğ’Œ = ğŸ“) i dokÅ‚adnoÅ›Ä‡ wyznaczenia ekstremum (zerowania siÄ™ gradientu) Îµ (np. $\mathcal{Îµ}={10^-3}$).
2. ObliczyÄ‡ w punkcie $\mathcal{x_i}$ wartoÅ›Ä‡ funkcji celu $\mathcal{f(x_i)}$ i jej gradientu $\mathcal{g(x_i)}$.
3. WyznaczyÄ‡ kierunek poszukiwaÅ„ przeciwny do kierunku gradientu $\mathcal{d}={-g(x_i)}$.
4. WykonaÄ‡ z punktu $\mathcal{x_i}$ krok o dÅ‚ugoÅ›ci e w wyznaczonym kierunku d, przechodzÄ…c do punktu $\mathcal{x_i+1}={x_i+e*d}$.
5. ObliczyÄ‡ wartoÅ›Ä‡ funkcji celu. JeÅ›li $\mathcal{f(x_i+1)}>={f(x_i)}$ , dokonaÄ‡ redukcji kroku (mnoÅ¼Ä…c jego wartoÅ›Ä‡ przez a) i ponowiÄ‡ prÃ³by. Po k niepomyÅ›lnych prÃ³bach zakoÅ„czyÄ‡ postÄ™powanie.
6. ObliczyÄ‡ wartoÅ›Ä‡ gradientu w nowym punkcie $\mathcal{g(x_i+1)}$. JeÅ›li $\mathcal{|g^T*g|>Îµ}$, przyjÄ…Ä‡ ğ’Š = ğ’Š + ğŸ i przejÅ›Ä‡ do
punktu 2. W przeciwnym razie zakoÅ„czyÄ‡ postÄ™powanie.


**3. Metoda najszybszego spadku**
Metoda najszybszego spadku stanowi modyfikacjÄ™ metody gradientu prostego. W metodzie
najszybszego spadku po okreÅ›leniu kierunku poszukiwaÅ„ wyznaczane jest minimum funkcji w tym
kierunku, a nie przesuniÄ™cie ze staÅ‚ym krokiem. WaÅ¼nÄ… wÅ‚asnoÅ›ciÄ… metody najszybszego spadku jest to, Å¼e
przy jej zastosowaniu kaÅ¼dy nowy kierunek poruszania siÄ™ ku optimum jest ortogonalny do poprzedniego.
TÅ‚umaczy siÄ™ to tym, Å¼e poruszanie siÄ™ w jednym kierunku trwa do tego czasu, dopÃ³ki kierunek ten nie
okaÅ¼e siÄ™ stycznym do jakiejÅ› linii staÅ‚ej wartoÅ›ci funkcji celu.

![Photo](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Metoda_najszybszego_spadku.svg/1280px-Metoda_najszybszego_spadku.svg.png)


**Algorytm metody najszybszego spadku:**
1. PrzyjÄ…Ä‡ punkt startowy $\mathcal{x_0}$, i dokÅ‚adnoÅ›Ä‡ wyznaczenia ekstremum (zerowania siÄ™ gradientu) Îµ (np. $\mathcal{Îµ}={10^-3}$).
2. ObliczyÄ‡ w punkcie $\mathcal{x_i}$ wartoÅ›Ä‡ funkcji celu $\mathcal{f(x_i)}$ i jej gradientu $\mathcal{g(x_i)}$.
3. WyznaczyÄ‡ kierunek poszukiwaÅ„ przeciwny do kierunku gradientu $\mathcal{d}={-g(x_i)}$.
4. WykonaÄ‡ z punktu $\mathcal{x_i}$ krok w wyznaczonym kierunku d, o takiej dÅ‚ugoÅ›ci e, by osiÄ…gnÄ…Ä‡ minimum w tym kierunku, przechodzÄ…c do punktu $\mathcal{x_i+1}={x_i+e*d}$.
5. ObliczyÄ‡ wartoÅ›Ä‡ funkcji celu i jej gradientu w nowym punkcie.
6. JeÅ›li $\mathcal{g^T*g>Îµ}$, przyjÄ…Ä‡ ğ’Š = ğ’Š + ğŸ i przejÅ›Ä‡ do punktu 2. W przeciwnym razie zakoÅ„czyÄ‡ postÄ™powanie.

**Uwaga**
W punkcie 4. powyÅ¼szego algorytmu naleÅ¼y rozwiÄ…zaÄ‡ zadanie optymalizacji funkcji jednej zmiennej min $\mathcal{f(e)=f(x_i+e*d)}$. Do utworzenia funkcji ğ‘“(ğ‘’) wygodnie jest siÄ™ posÅ‚uÅ¼yÄ‡ poleceniem matlabFunction, ktÃ³re pozwala na przeksztaÅ‚cenie wyraÅ¼enia symbolicznego w funkcjÄ™ anonimowÄ…. 

Do obliczania minimum funkcji ğ‘“(ğ‘’) zastosowaÄ‡ moÅ¼na jednÄ… z metod poszukiwania minimum funkcji jednej zmiennej, ktÃ³re zostaÅ‚y zaimplementowane na poprzednim laboratorium. NiektÃ³re z tych metod wymagajÄ… wyznaczenia przedziaÅ‚u, na ktÃ³rym spodziewamy siÄ™ znaleÅºÄ‡ minimum. W tym celu skorzystaÄ‡ moÅ¼na z heurystycznej metody opracowanej przez W. Swanna, wedÅ‚ug ktÃ³rej (k+1) punkt okreÅ›la siÄ™ z rekurencyjnego wzoru:
$\mathcal{x_k+1}={x_k+2^k*\bigtriangleup}$, ğ‘˜ = 0,1,2,3, â€¦ , ğ‘™

gdzie: 

$\mathcal{x_0}$ - punkt poczÄ…tkowy
$\mathcal{\bigtriangleup}$ â€“ dÅ‚ugoÅ›Ä‡ kroku (staÅ‚a, np. $\mathcal{\bigtriangleup}={10^-2}$.

Znak $\mathcal{\bigtriangleup}$ okreÅ›la siÄ™ drogÄ… porÃ³wnaÅ„ wielkoÅ›ci $\mathcal{f(x_0)+\bigtriangleup}$ i $\mathcal{f(x_0)-\bigtriangleup}$. JeÅ¼eli $\mathcal{f(x_0)-\bigtriangleup}>{f(x_0)+\bigtriangleup}$, to zgodnie z zaÅ‚oÅ¼eniem o wypukÅ‚oÅ›ci w dÃ³Å‚, punkt minimum powinien siÄ™ znajdowaÄ‡ na prawo od punktu $\mathcal{x_0}$ i wielkoÅ›Ä‡ $\mathcal{\bigtriangleup}$ powinna byÄ‡ wybierana jako dodatnia. JeÅ¼eli zmieniÄ‡ znak nierÃ³wnoÅ›ci na przeciwny, to $\mathcal{\bigtriangleup}$ naleÅ¼y wybraÄ‡ jako wielkoÅ›Ä‡ ujemnÄ…. JeÅ¼eli $\mathcal{f(x_l)}>={f(x_0)}$, to punkt minimum leÅ¼y miÄ™dzy $\mathcal{f(x_0)}$ i $\mathcal{f(x_l)}$ i poszukiwanie punktÃ³w granicznych zostaje zakoÅ„czone.